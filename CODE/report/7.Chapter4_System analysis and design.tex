\section{Thiết kế hệ thống}
\label{chap:design}
\label{chap:thietke}

\begin{indentParagraph}
Chương này trình bày chi tiết thiết kế hệ thống truy xuất thông tin đa phương thức từ âm thanh. Nội dung bao gồm kiến trúc tổng thể, thiết kế các pipeline xử lý, sơ đồ module và luồng dữ liệu. Đặc biệt, chương đi sâu vào thiết kế hệ thống chống hallucination - một trong những đóng góp quan trọng của đề tài.
\end{indentParagraph}

\subsection{Kiến trúc tổng thể}

Hệ thống được thiết kế theo kiến trúc module hóa, cho phép mở rộng và thay thế các thành phần một cách linh hoạt.

\begin{table}[H]
    \centering
    \caption{Các thành phần chính của hệ thống}
    \label{tab:main_components}
    \begin{tabular}{|l|p{5cm}|p{6cm}|}
        \hline
        \textbf{Thành phần} & \textbf{Chức năng} & \textbf{Công nghệ} \\
        \hline
        ASR Module & Chuyển đổi audio thành văn bản với timestamp & Faster-Whisper, Silero VAD \\
        \hline
        Document Processor & Xử lý đa định dạng file & PyMuPDF, PaddleOCR, python-docx \\
        \hline
        Post Processor & Làm sạch và chuẩn hóa văn bản & Ollama/Gemini LLM \\
        \hline
        Chunking Module & Phân đoạn văn bản & Semantic/Recursive/Fixed \\
        \hline
        Embedding Module & Tạo vector biểu diễn & SBERT, E5, OpenAI, Google \\
        \hline
        Vector Database & Lưu trữ và tìm kiếm vector & Qdrant + BM25 \\
        \hline
        RAG System & Sinh câu trả lời từ ngữ cảnh & Ollama/Gemini LLM \\
        \hline
        Answer Verifier & Xác minh độ tin cậy & LLM-based grounding check \\
        \hline
        Conflict Detector & Phát hiện mâu thuẫn & Date-aware comparison \\
        \hline
        Voice Input & Thu âm và nhận dạng câu hỏi bằng giọng nói & Audio-Recorder + ASR \\
        \hline
        TTS Module & Chuyển văn bản thành giọng nói & Edge-TTS \\
        \hline
    \end{tabular}
\end{table}

Hệ thống gồm 3 tầng: Presentation Layer (giao diện người dùng với Streamlit), Business Logic Layer (các module xử lý nghiệp vụ ASR, RAG, OCR), và Data Layer (lưu trữ dữ liệu Vector DB, File System, Cache).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{report/figures/system_architecture.png}
    \caption{Kiến trúc tổng thể hệ thống}
    \label{fig:system_architecture}
\end{figure}

\subsection{Pipeline xử lý tài liệu}
\label{sec:document_pipeline}

Hệ thống sử dụng kiến trúc two-step pipeline cho việc import tài liệu, cho phép tái sử dụng kết quả xử lý nặng khi cần re-index.

\subsubsection{Bước 1: Processing}

Bước này thực hiện các tác vụ xử lý nặng về mặt tính toán và lưu trữ kết quả thô để sử dụng cho các lần indexing sau. Đầu tiên, hệ thống copy file từ thư mục nguồn data/resource/ sang knowledge\_base/documents/ để đảm bảo tính độc lập của knowledge base. Tiếp theo, UnifiedProcessor phát hiện loại file dựa trên extension và chọn processor phù hợp. Tùy theo loại file, hệ thống thực hiện OCR với PDF scan và hình ảnh hoặc ASR với audio và video để trích xuất nội dung văn bản. Kết quả xử lý thô được lưu vào knowledge\_base/processed/*.json kèm theo metadata như timestamp, duration và word count.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{report/figures/processing_pipeline.png}
    \caption{Pipeline xử lý tài liệu - Bước 1 (Processing)}
    \label{fig:processing_pipeline}
\end{figure}

\subsubsection{Bước 2: Indexing}

Bước này sử dụng kết quả thô từ bước 1 để tạo index có thể tìm kiếm được trong vector database. Hệ thống bắt đầu bằng việc đọc RAW content từ các file processed/*.json đã được tạo ở bước trước. Nội dung thô sau đó được đưa qua module post-processing sử dụng LLM để sửa lỗi chính tả và chuẩn hóa văn bản, đặc biệt quan trọng với kết quả từ ASR và OCR. Văn bản đã được làm sạch được chia thành các chunks với kích thước phù hợp, mỗi chunk giữ nguyên metadata như timestamp, source file và vị trí trong tài liệu gốc. Cuối cùng, embedding module tạo vector biểu diễn cho mỗi chunk và upload vào Qdrant để phục vụ tìm kiếm.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{report/figures/indexing_pipeline.png}
    \caption{Pipeline xử lý tài liệu - Bước 2 (Indexing)}
    \label{fig:indexing_pipeline}
\end{figure}

\subsubsection{Cấu trúc dữ liệu processed}

Mỗi tài liệu sau khi xử lý được lưu dưới dạng JSON với cấu trúc:

\begin{lstlisting}[language=json, caption=Cau truc processed document, label=lst:processed_json]
{
    "doc_id": "550e8400-e29b-41d4-a716-446655440000",
    "original_filename": "lecture_01.mp3",
    "file_type": "audio",
    "source_path": "knowledge_base/documents/audio/lecture_01.mp3",
    "processed_at": "2024-12-15T10:30:00",
    "raw_content": "Xin chao cac em sinh vien...",
    "metadata": {
        "duration": 3600,
        "language": "vi",
        "word_count": 5234
    },
    "timestamps": [
        {"start": 0.0, "end": 2.5, "text": "Xin chao cac em sinh vien"},
        {"start": 2.5, "end": 5.0, "text": "Hom nay chung ta hoc ve..."}
    ]
}
\end{lstlisting}

\subsection{Pipeline truy vấn}
\label{sec:query_pipeline}

Pipeline truy vấn xử lý câu hỏi của người dùng và sinh câu trả lời có nguồn trích dẫn. Người dùng có thể nhập câu hỏi bằng văn bản hoặc bằng giọng nói thông qua Voice Input.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{report/figures/query_pipeline.png}
    \caption{Pipeline truy vấn}
    \label{fig:query_pipeline}
\end{figure}

\subsubsection{Query Expansion}
Module Query Expansion mở rộng câu hỏi gốc để cải thiện recall bằng cách sử dụng LLM sinh 2-3 biến thể câu hỏi với cùng ngữ nghĩa. Sau đó thực hiện retrieval song song cho tất cả các biến thể, rồi kết hợp kết quả bằng RRF hoặc score fusion.
% Mở rộng câu hỏi để cải thiện recall:

% \begin{lstlisting}[language=Python, caption=Query Expansion, label=lst:query_expansion]
% def expand_query(query: str, llm) -> List[str]:
%     """Generate query variations for better retrieval."""
%     prompt = f"""
%     Given the question: "{query}"
%     Generate 2-3 alternative phrasings that capture the same intent.
%     Return as JSON array.
%     """
%     variations = llm.generate(prompt)
%     return [query] + variations
% \end{lstlisting}

\textbf{Tối ưu hóa Query Expansion:} Với mô hình LLM cục bộ tầm trung (7B parameters), việc sinh 2-3 biến thể câu hỏi có thể làm tăng latency đáng kể (thêm 1-2 giây). Để đảm bảo yêu cầu phi chức năng về thời gian phản hồi dưới 10 giây (NFR1), hệ thống áp dụng hai chiến lược tối ưu: (1) \textbf{Conditional Expansion} - chỉ kích hoạt query expansion khi kết quả retrieval ban đầu có max score thấp dưới ngưỡng 0.6, tức là khi query gốc không tìm được kết quả tốt; (2) \textbf{Parallel Retrieval} - khi expansion được kích hoạt, các query biến thể được gửi đến vector database song song (parallel) thay vì tuần tự, tận dụng async I/O để giảm thiểu thời gian chờ.

% \textit{Lưu ý:} Module Query Expansion được thiết kế như \textbf{optional feature} và mặc định tắt để ưu tiên tốc độ phản hồi. Người dùng có thể bật tính năng này thông qua biến môi trường \texttt{ENABLE\_QUERY\_EXPANSION=true} khi cần cải thiện recall cho các truy vấn phức tạp.

\subsubsection{Hybrid Search}

Kết hợp vector search và BM25:

\begin{equation}
    \text{score}_{hybrid}(d) = \alpha \cdot \text{score}_{vector}(d) + (1-\alpha) \cdot \text{score}_{bm25}(d)
    \label{eq:hybrid_score}
\end{equation}

Ngoài ra, hệ thống hỗ trợ Reciprocal Rank Fusion (RRF) như đã trình bày ở Chương \ref{chap:cosoly}.
% Hoặc sử dụng Reciprocal Rank Fusion (RRF):

% \begin{equation}
%     \text{RRF}(d) = \sum_{r \in \{vector, bm25\}} \frac{1}{k + rank_r(d)}
%     \label{eq:rrf_search}
% \end{equation}

\subsubsection{Reranking và tránh thiên kiến}
Sau bước Hybrid Search, hệ thống sử dụng \textbf{Cross-Encoder} để rerank kết quả. Điểm thiết kế quan trọng là sự \textbf{phân tách mô hình}: Reranking sử dụng Cross-Encoder chuyên biệt, Answer Generation sử dụng LLM, và Answer Verification sử dụng LLM với prompt template khác. Việc phân tách này giúp giảm thiểu rủi ro \textbf{thiên kiến tự đánh giá} (self-evaluation bias) - Cross-Encoder đánh giá relevance độc lập, trong khi Verification prompt được thiết kế để ``phản biện'' câu trả lời thay vì xác nhận.

% Sau bước Hybrid Search, hệ thống sử dụng \textbf{Cross-Encoder} để rerank kết quả trước khi đưa vào LLM sinh câu trả lời. Một điểm thiết kế quan trọng là sự phân tách giữa các mô hình:

% \begin{itemize}
%     \item \textbf{Reranking:} Sử dụng mô hình Cross-Encoder chuyên biệt (mặc định \texttt{cross-encoder/ms-marco-MiniLM-L-6-v2}, hỗ trợ \texttt{BAAI/bge-reranker-base} cho multilingual) - đây là mô hình được huấn luyện riêng cho tác vụ đánh giá relevance giữa query và document.
%     \item \textbf{Answer Generation:} Sử dụng LLM (Ollama/Gemini) để sinh câu trả lời từ context đã được rerank.
%     \item \textbf{Answer Verification:} Sử dụng LLM với prompt template hoàn toàn khác (\texttt{verification\_prompt}) để kiểm chứng câu trả lời.
% \end{itemize}

% Việc sử dụng các mô hình và prompt khác nhau cho từng bước giúp giảm thiểu rủi ro \textbf{thiên kiến tự đánh giá} (self-evaluation bias) - tình huống mô hình ``tự khen'' kết quả của chính mình. Cross-Encoder đánh giá relevance một cách độc lập, trong khi Verification prompt được thiết kế để ``phản biện'' câu trả lời thay vì xác nhận.

\subsection{Thiết kế module ASR}
\label{sec:asr_design}

\subsubsection{Kiến trúc ASR Module}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{report/figures/asr_class.png}
    \caption{Class diagram của ASR Module}
    \label{fig:asr_class}
\end{figure}

\subsubsection{Quy trình xử lý audio}

Quy trình xử lý audio bắt đầu với việc load file và chuẩn hóa định dạng về 16kHz mono để đảm bảo tương thích với mô hình Whisper. Silero VAD sau đó phân tích toàn bộ audio để phát hiện các đoạn chứa giọng nói, loại bỏ các khoảng im lặng không cần thiết. Dựa trên kết quả VAD, audio được chia thành các đoạn ngắn với ranh giới tự nhiên theo ngữ cảnh. Mỗi đoạn được đưa qua Whisper để transcribe, với kết quả bao gồm văn bản và word-level timestamps. Kết quả từ các đoạn được ghép lại với việc điều chỉnh timestamp để tạo thành transcript hoàn chỉnh. Bước post-processing sử dụng LLM để sửa lỗi chính tả là tùy chọn, có thể bật tắt tùy theo yêu cầu về chất lượng và thời gian xử lý.\\

\begin{lstlisting}[language=Python, caption=ASR Output Format, label=lst:asr_output]
@dataclass
class TranscriptSegment:
    start: float          # Start time in seconds
    end: float            # End time in seconds
    text: str             # Transcribed text
    confidence: float     # Confidence score (0-1)
    words: List[WordInfo] # Word-level timestamps

@dataclass
class TranscriptResult:
    segments: List[TranscriptSegment]
    language: str
    duration: float
    full_text: str
\end{lstlisting}

\subsection{Thiết kế hệ thống chống Hallucination}
\label{sec:anti_hallucination}

\subsubsection{Kiến trúc Anti-Hallucination}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{report/figures/anti_hallucination.png}
    \caption{Kiến trúc hệ thống chống Hallucination}
    \label{fig:anti_hallucination}
\end{figure}

\subsubsection{Answer Verification}

Module xác minh câu trả lời dựa trên ngữ cảnh nguồn:

\begin{lstlisting}[language=Python, caption=Answer Verifier, label=lst:answer_verifier]
class AnswerVerifier:
    def verify(self, answer: str, sources: List[str]) -> VerificationResult:
        prompt = f"""
        Answer: {answer}
        Sources: {sources}

        Check if each claim in the answer is supported by sources.
        Return verification level and evidence.
        """
        return self.llm.analyze(prompt)
\end{lstlisting}

Các mức độ grounding:

\begin{table}[H]
    \centering
    \caption{Các mức độ grounding}
    \label{tab:grounding_levels}
    \begin{tabular}{|l|p{8cm}|}
        \hline
        \textbf{Mức độ} & \textbf{Mô tả} \\
        \hline
        FULLY\_GROUNDED & Tất cả thông tin trong câu trả lời đều có trong nguồn \\
        PARTIALLY\_GROUNDED & Một phần thông tin có trong nguồn, phần còn lại không xác minh được \\
        LIKELY\_HALLUCINATED & Thông tin mâu thuẫn với nguồn hoặc không có cơ sở \\
        UNVERIFIABLE & Không đủ nguồn để xác minh \\
        \hline
    \end{tabular}
\end{table}

\textbf{Xử lý PARTIALLY\_GROUNDED:} Khi kết quả verification là \texttt{PARTIALLY\_GROUNDED}, hệ thống áp dụng chiến lược \texttt{accept\_with\_warning}: câu trả lời vẫn được trả về nhưng kèm theo cảnh báo rõ ràng cho người dùng về độ tin cậy. Cụ thể, giao diện sẽ hiển thị thông báo highlight các claims chưa có căn cứ. Cách tiếp cận này cân bằng giữa tính hữu ích và tính minh bạch, thay vì từ chối hoàn toàn hoặc tốn thêm thời gian xử lý với vòng lặp tinh lọc.

\subsubsection{Conflict Detection}

Module phát hiện mâu thuẫn giữa các nguồn:

\begin{lstlisting}[language=Python, caption=Conflict Detector, label=lst:conflict_detector]
class ConflictDetector:
    def __init__(self, strategy: str = "PREFER_NEWER"):
        self.strategy = strategy  # PREFER_NEWER, PREFER_HIGHER_SCORE,
                                  # MERGE_ALL, SHOW_ALL_VERSIONS

    def detect(self, sources: List[Document]) -> List[Conflict]:
        # Compare key facts across sources
        facts = self._extract_facts(sources)
        conflicts = self._find_contradictions(facts)
        # Apply resolution strategy
        for conflict in conflicts:
            conflict.resolution = self._resolve(conflict)

        return conflicts
\end{lstlisting}

Hệ thống hỗ trợ bốn chiến lược giải quyết mâu thuẫn: \textbf{PREFER\_NEWER} ưu tiên tài liệu mới hơn (mặc định), \textbf{PREFER\_HIGHER\_SCORE} ưu tiên tài liệu có relevance score cao hơn, \textbf{MERGE\_ALL} kết hợp thông tin từ tất cả nguồn, và \textbf{SHOW\_ALL\_VERSIONS} hiển thị tất cả phiên bản cho người dùng tự quyết định.

\subsubsection{Safe Abstention}

Module quyết định từ chối trả lời khi không đủ thông tin:

\begin{lstlisting}[language=Python, caption=Abstention Checker, label=lst:abstention]
class AbstentionChecker:
    def should_abstain(self, query: str, sources: List[str],
                       confidence: float) -> Tuple[bool, str]:
        # Check 1: No relevant sources found
        if not sources:
            return True, "Khong tim thay thong tin lien quan"

        # Check 2: Low confidence
        if confidence < self.min_confidence:
            return True, "Do tin cay khong du cao"

        # Check 3: Query outside knowledge base scope
        if self._is_out_of_scope(query, sources):
            return True, "Cau hoi nam ngoai pham vi co so tri thuc"

        return False, ""
\end{lstlisting}

\subsubsection{Prompt Engineering cho Anti-Hallucination}

Hệ thống sử dụng các prompt template đặc biệt:

\begin{lstlisting}[caption=Strict QA Prompt Template, label=lst:strict_qa]
STRICT_QA_PROMPT = """
You are a helpful assistant that answers questions based ONLY on
the provided context. Follow these rules strictly:

1. ONLY use information from the context below
2. If the context doesn't contain the answer, say "Toi khong tim
   thay thong tin nay trong tai lieu"
3. NEVER make up information
4. Quote relevant parts from the context when possible
5. If information is uncertain, express uncertainty

Context:
{context}

Question: {query}

Answer (in Vietnamese):
"""
\end{lstlisting}

\subsection{Thiết kế module Chunking}
\label{sec:chunking_design}

\subsubsection{Các phương pháp chunking}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{report/figures/chunking_class.png}
    \caption{Class diagram của Chunking Module}
    \label{fig:chunking_class}
\end{figure}

\subsubsection{Semantic Chunking}

Hệ thống sử dụng \texttt{SemanticChunker} từ thư viện \texttt{langchain-experimental} để thực hiện phân đoạn dựa trên ngữ nghĩa. Thuật toán hoạt động như sau:

\begin{enumerate}
    \item \textbf{Tách câu:} Văn bản được tách thành các câu riêng lẻ
    \item \textbf{Tạo embedding:} Mỗi câu được chuyển thành vector embedding sử dụng mô hình đã cấu hình (SBERT/E5)
    \item \textbf{Tính similarity:} Tính cosine similarity giữa các câu liền kề
    \item \textbf{Phát hiện breakpoint:} Các điểm có similarity thấp (thay đổi chủ đề) được đánh dấu là ranh giới chunk
    \item \textbf{Tạo chunk:} Nhóm các câu liên tiếp giữa các breakpoint thành chunk
\end{enumerate}

Hệ thống sử dụng \texttt{SemanticChunker} từ thư viện \texttt{langchain-experimental} với tham số \texttt{breakpoint\_threshold\_amount=95}, nghĩa là chỉ các điểm có similarity thấp hơn 95\% các điểm khác mới được coi là ranh giới chunk, đảm bảo chunk không bị chia quá nhỏ.

% \begin{lstlisting}[language=Python, caption=Khoi tao Semantic Chunker, label=lst:semantic_chunking]
% from langchain_experimental.text_splitter import SemanticChunker

% # Khoi tao voi local embedding
% chunker = SemanticChunker(
%     embeddings=embedding_model,
%     breakpoint_threshold_type="percentile",
%     breakpoint_threshold_amount=95
% )

% # Chunk van ban
% chunks = chunker.split_text(document_text)
% \end{lstlisting}

% Tham số \texttt{breakpoint\_threshold\_amount=95} có nghĩa là chỉ các điểm có similarity thấp hơn 95\% các điểm khác mới được coi là ranh giới chunk, đảm bảo chunk không bị chia quá nhỏ.

\subsubsection{Chunk với Timestamp}

Đối với audio transcript, chunk giữ nguyên thông tin timestamp:

\begin{lstlisting}[language=Python, caption=Chunk voi Timestamp, label=lst:chunk_timestamp]
@dataclass
class AudioChunk:
    content: str
    start_time: float  # seconds
    end_time: float    # seconds
    source_file: str
    chunk_index: int

    def get_timestamp_str(self) -> str:
        """Format: [00:01:23 - 00:02:45]"""
        return f"[{self._format_time(self.start_time)} - " \
               f"{self._format_time(self.end_time)}]"
\end{lstlisting}

Hệ thống sử dụng \textbf{segment-level chunking}, mỗi segment là một câu hoàn chỉnh từ ASR, đảm bảo không cắt đôi câu và timestamp luôn chính xác với nội dung.

\subsection{Thiết kế cơ sở dữ liệu}
\label{sec:database_design}

% \subsubsection{Schema Vector Database}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{report/figures/qdrant_schema.png}
    \caption{Schema Qdrant Collection}
    \label{fig:qdrant_schema}
\end{figure}

\textbf{Knowledge Base Index: } File \texttt{knowledge\_base/index.json} lưu trữ metadata của tất cả tài liệu. Ngoài ra còn có thống kê tổng hợp như số lượng tài liệu, chunks và phân loại theo định dạng file.

% File \texttt{knowledge\_base/index.json} lưu trữ metadata của tất cả tài liệu:

% \begin{lstlisting}[language=json, caption=Knowledge Base Index, label=lst:kb_index]
% {
%     "documents": {
%         "doc_001": {
%             "original_filename": "lecture_ai.mp3",
%             "file_type": "audio",
%             "source_path": "documents/audio/lecture_ai.mp3",
%             "processed_path": "processed/doc_001.json",
%             "created_at": "2024-12-15T10:30:00",
%             "document_date": "2024-09-01",
%             "chunk_count": 45,
%             "status": "indexed"
%         }
%     },
%     "statistics": {
%         "total_documents": 150,
%         "total_chunks": 3420,
%         "by_type": {
%             "audio": 50,
%             "pdf": 80,
%             "docx": 20
%         }
%     }
% }
% \end{lstlisting}

% \textit{Lưu ý:} Trường \texttt{document\_date} (ngày ban hành tài liệu) khác với \texttt{created\_at} (ngày upload vào hệ thống). Điều này quan trọng cho chiến lược \texttt{PREFER\_NEWER} trong Conflict Detection, vì một tài liệu được upload năm 2025 có thể chứa quy định từ năm 2020.
% Hệ thống sử dụng \texttt{document\_date} (nếu có) hoặc fallback về \texttt{created\_at} khi cần so sánh độ mới của thông tin.

\textbf{Post-Processing Cache:} Cache sử dụng MD5 hash của \texttt{method:model:content} làm key để tránh xử lý lại nội dung đã post-process. Mỗi entry lưu trữ thông tin về phương thức xử lý, model sử dụng, thời gian tạo và đường dẫn đến file kết quả. Hệ thống theo dõi thống kê cache hits/misses để đánh giá hiệu quả.

% Cache được thiết kế để tránh xử lý lại nội dung đã post-process:

% \begin{lstlisting}[language=json, caption=Cache Index, label=lst:cache_index]
% {
%     "entries": {
%         "a1b2c3d4e5f6": {
%             "hash": "md5_of_method:model:content",
%             "method": "ollama",
%             "model": "qwen2.5:7b",
%             "created_at": "2024-12-15T10:00:00",
%             "chunk_file": "chunks/a1b2c3d4e5f6.json"
%         }
%     },
%     "statistics": {
%         "total_entries": 1500,
%         "cache_hits": 12000,
%         "cache_misses": 500
%     }
% }
% \end{lstlisting}

\subsection{Thiết kế giao diện}
\label{sec:ui_design}

\subsubsection{Student Portal}

Giao diện Student Portal được thiết kế với hai phương thức nhập câu hỏi: văn bản và giọng nói. Khi người dùng chọn Voice Input, hệ thống ghi âm qua microphone, sử dụng ASR để chuyển đổi thành văn bản, hiển thị transcript để xác nhận, sau đó xử lý truy vấn và tự động đọc câu trả lời bằng TTS. Với các nguồn từ audio/video, hệ thống hiển thị thông tin timestamp trong phần trích dẫn nguồn, cho phép sinh viên biết chính xác vị trí trong file gốc mà thông tin được trích xuất.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{report/figures/student_wireframe.png}
    \caption{Wireframe Student Portal}
    \label{fig:student_wireframe}
\end{figure}

\subsubsection{Admin Portal}
Admin Portal cung cấp giao diện quản lý tài liệu cho người quản trị với các chức năng chính: upload tài liệu mới (hỗ trợ kéo thả nhiều file), xem danh sách tài liệu đã import kèm thông tin metadata (loại file, số chunks, ngày tạo), xóa tài liệu khỏi knowledge base, và re-index lại tài liệu khi cần thay đổi cấu hình chunking hoặc embedding. Giao diện cũng hiển thị thống kê tổng quan về knowledge base như tổng số tài liệu, chunks và phân bố theo định dạng file.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{report/figures/admin_wireframe.png}
%     \caption{Wireframe Admin Portal}
%     \label{fig:admin_wireframe}
% \end{figure}

\newpage
\subsection*{Tổng kết thiết kế}

Chương này đã trình bày thiết kế chi tiết của hệ thống truy xuất thông tin đa phương thức từ âm thanh. Kiến trúc hệ thống được xây dựng theo mô hình 3 tầng gồm Presentation Layer cho giao diện người dùng, Business Logic Layer cho các module xử lý nghiệp vụ, và Data Layer cho lưu trữ dữ liệu. Mô hình này cho phép các tầng phát triển độc lập và dễ dàng mở rộng khi cần thiết.

Một trong những điểm đặc biệt của thiết kế là kiến trúc two-step pipeline tách biệt giai đoạn processing (OCR/ASR) khỏi giai đoạn indexing (chunking/embedding). Kiến trúc này cho phép tái sử dụng kết quả xử lý nặng khi thay đổi cấu hình indexing, kết hợp với cơ chế caching MD5 hash giúp tiết kiệm đáng kể thời gian khi re-index tài liệu.

Thiết kế hệ thống Anti-Hallucination là đóng góp quan trọng của đề tài, bao gồm ba cơ chế phối hợp: Answer Verification kiểm tra tính chính xác của câu trả lời dựa trên nguồn, Conflict Detection phát hiện và xử lý mâu thuẫn giữa các tài liệu, và Safe Abstention từ chối trả lời khi không có đủ thông tin đáng tin cậy. Ba cơ chế này đảm bảo câu trả lời của hệ thống luôn có căn cứ và đáng tin cậy.

Giao diện người dùng được thiết kế với hai portal phục vụ hai nhóm đối tượng khác nhau. Student Portal cung cấp khả năng tra cứu thông tin qua cả văn bản và giọng nói, kết hợp với TTS để tạo trải nghiệm hỏi đáp hoàn toàn bằng âm thanh. Admin Portal cung cấp các công cụ quản lý tài liệu bao gồm upload, xem danh sách, xóa và re-index.

\newpage
