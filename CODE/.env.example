# ========================
# API Keys (chon 1 trong 2 hoac ca 2)
# ========================
# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Google API Key (Gemini)
GOOGLE_API_KEY=your_google_api_key_here

# ========================
# Provider Selection
# ========================
# LLM Provider: "ollama" (local), "openai", hoac "google"
# Recommended: ollama (mien phi, chay offline)
LLM_PROVIDER=ollama

# Embedding Provider: "local" (Sentence-BERT/E5), "openai", hoac "google"
# Recommended: local (khong can API key, mien phi)
EMBEDDING_PROVIDER=local

# ========================
# Qdrant Configuration
# ========================
# Local Qdrant (Docker hoac in-memory)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Qdrant Cloud (optional - neu dung cloud thi comment local config)
# QDRANT_URL=https://your-cluster.qdrant.io
# QDRANT_API_KEY=your_qdrant_api_key

# Collection name
COLLECTION_NAME=audio_transcripts

# ========================
# Model Configurations
# ========================
# Whisper ASR Model
WHISPER_ENGINE=faster           # faster (4x speed), openai (original), auto
WHISPER_MODEL=base              # tiny, base, small, medium, large-v2, large-v3
WHISPER_DEVICE=cpu              # cuda, cpu, auto
WHISPER_COMPUTE_TYPE=auto       # float16 (GPU), int8 (CPU), auto (faster-whisper only)

# VAD (Voice Activity Detection) - faster-whisper only
# Skips silence, reduces hallucination, faster processing
WHISPER_VAD_FILTER=false        # true to enable VAD
WHISPER_VAD_THRESHOLD=0.5       # Speech probability threshold (0-1)
WHISPER_VAD_MIN_SPEECH_MS=250   # Minimum speech duration to keep (ms)
WHISPER_VAD_MIN_SILENCE_MS=500  # Minimum silence to split segments (ms)
WHISPER_VAD_SPEECH_PAD_MS=400   # Padding around speech segments (ms)

# OpenAI Models (khi dung provider=openai)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBEDDING_DIMENSION=1536
OPENAI_LLM_MODEL=gpt-4o-mini

# Google Models (khi dung provider=google)
GOOGLE_EMBEDDING_MODEL=models/text-embedding-004
GOOGLE_EMBEDDING_DIMENSION=768
GOOGLE_LLM_MODEL=gemini-2.0-flash

# Local Embedding Models (khi dung EMBEDDING_PROVIDER=local)
# Supported models:
#   - sbert (default): sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768d)
#   - e5: intfloat/multilingual-e5-base (768d)
#   - e5-large: intfloat/multilingual-e5-large (1024d)
#   - vi-sbert: keepitreal/vietnamese-sbert (768d)
LOCAL_EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
LOCAL_EMBEDDING_DIMENSION=768
# LOCAL_EMBEDDING_DEVICE=cuda  # or cpu (auto-detect if not set)

# ========================
# Ollama Configuration (khi dung LLM_PROVIDER=ollama)
# ========================
# Cai dat Ollama: https://ollama.ai/download
# Chay: ollama serve
# Tai model: ollama pull qwen2.5:3b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b       # Model cho RAG queries
OLLAMA_AUTO_START=true        # Tu dong start Ollama neu chua chay

# Supported Ollama models:
#   - llama3.2 (default): Fast, 3B params
#   - llama3.2:1b: Very fast, 1B params
#   - llama3.1: Better quality, 8B params
#   - mistral: Good balance, 7B params
#   - qwen2.5: Excellent multilingual/Vietnamese, 7B params
#   - qwen2.5:3b: Fast multilingual, 3B params
#   - gemma2:2b: Very fast, 2B params
#   - phi3: Efficient reasoning, 3.8B params

# LLM Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=500

# ========================
# Chunking Parameters
# ========================
CHUNK_SIZE=500
CHUNK_OVERLAP=50
CHUNKING_METHOD=semantic

# Semantic chunking (khi method=semantic)
SEMANTIC_THRESHOLD=0.65
SEMANTIC_WINDOW_SIZE=5

# ========================
# Retrieval Parameters
# ========================
TOP_K=5
SIMILARITY_THRESHOLD=0.5
MMR_LAMBDA=0.5

# ========================
# PDF Processing Configuration
# ========================
# Hybrid mode: smart text extraction + OCR only for image regions
PDF_HYBRID_MODE=true            # true (recommended), false (legacy full-page OCR)
PDF_MIN_IMAGE_SIZE=50           # Min image dimension (px) to OCR

# ========================
# OCR Configuration (for scanned PDFs and images)
# ========================
# OCR Engine: paddleocr (recommended for Vietnamese), easyocr
OCR_ENGINE=paddleocr

# Language: vi (Vietnamese), en (English), ch (Chinese)
OCR_LANGUAGE=vi

# Use GPU for OCR (faster if available)
OCR_USE_GPU=true

# DPI for PDF rendering (higher = better quality but slower)
# Recommended: 300 for good balance, 150 for speed, 400 for best quality
OCR_DPI=400

# Max image dimension (pixels) - resize larger images to prevent crashes
# PaddleOCR limit ~4000px, use 3500 for safety margin
OCR_MAX_IMAGE_SIZE=3500

# Minimum confidence threshold (0.0 - 1.0)
# Higher = fewer false positives, lower = more text detected
OCR_MIN_CONFIDENCE=0.5

# Enable preprocessing (deskew, denoise, binarize)
OCR_PREPROCESS=true

# ========================
# Post-Processing Configuration
# ========================
# Automatically fix Vietnamese text errors after extraction
#
# Extraction types:
#   - DIRECT: Text extracted directly (doc, txt, md, code, etc.)
#   - INDIRECT: Text from OCR (pdf, images) or ASR (audio, video)
#
# Processing methods:
#   - none: No processing (fastest)
#   - transformer: bmd1905/vietnamese-correction-v2 (HuggingFace model)
#   - ollama: qwen2.5:7b with Vietnamese correction prompt (best quality)

# Method for DIRECT extraction (doc, txt, etc.)
# Default: none (text is usually clean)
POSTPROCESS_DIRECT=none

# Method for INDIRECT extraction (OCR/ASR)
# Default: ollama (best for fixing OCR/ASR errors)
POSTPROCESS_INDIRECT=ollama

# Ollama model for post-processing
POSTPROCESS_OLLAMA_MODEL=qwen2.5:7b

# Transformer model for post-processing
POSTPROCESS_TRANSFORMER_MODEL=bmd1905/vietnamese-correction-v2

# ========================
# Post-Processing Cache
# ========================
# Cache post-processing results to avoid re-processing same text
# Useful when re-indexing documents
POSTPROCESS_CACHE=true
POSTPROCESS_CACHE_DIR=data/cache/post_processing

# HuggingFace token (optional, for private models)
HF_TOKEN=your_huggingface_token_here
